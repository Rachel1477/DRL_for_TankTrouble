# RL Agent 行为改进总结 v2.0

## 🎯 解决的问题

您观察到 agent 出现以下异常行为：
1. **反复前后犹豫运动** - 没有明确的移动策略
2. **莫名其妙的自杀** - 过度接近敌人导致被击杀
3. **朝不同方向攻击** - 射击时机和方向混乱

## 🚀 实施的三大改进

### 改进 1: 直线射击大奖励 (+10分，10秒冷却)

**问题**: Agent 不知道何时是射击的最佳时机

**解决方案**: 
- 当炮口对准敌人（±18度内）**且**两者之间没有墙壁阻挡时
- 射击可获得 +10 的巨大奖励
- 为避免spam，设置10秒冷却期（冷却中仍可获得+2普通精确射击奖励）

**代码位置**: `rl/TankEnv.cc` - `calculateReward()`

```cpp
// 直线射击检测
if (align_norm_fire < 0.1)  // 瞄准精确（18度内）
{
    if (hasDirectLineToEnemy(my.pos, en.pos))  // 无墙阻挡
    {
        if (current_time - last_direct_shot_time_ >= 10.0)  // 冷却完成
        {
            r += 10.0;  // 🎯 直线射击大奖励！
            last_direct_shot_time_ = current_time;
        }
        else
        {
            r += 2.0;  // 冷却中，普通精确射击奖励
        }
    }
}
```

**预期效果**:
- Agent 学会寻找"视线清晰"的射击位置
- 鼓励战术性走位，避免盲目冲锋
- 减少无效射击

---

### 改进 2: 频繁射击惩罚 (-1分/次)

**问题**: Agent 可能无脑连续射击，浪费弹药

**解决方案**:
- 追踪最近3秒内的所有射击
- 如果3秒内射击超过3次，每次额外射击扣除1分

**代码位置**: `rl/TankEnv.cc` - `calculateReward()`

```cpp
// 清理超过3秒的旧记录
while (!recent_shot_times_.empty() && 
       current_time - recent_shot_times_.front() > 3.0)
{
    recent_shot_times_.pop_front();
}

// 惩罚频繁射击
if (recent_shot_times_.size() > 3)
{
    r -= 1.0;  // 🚫 频繁射击惩罚
}
```

**预期效果**:
- Agent 学会控制射击节奏
- 减少"疯狂扫射"行为
- 更谨慎地选择射击时机

---

### 改进 3: 全局地图信息 (8x8网格 + 直线视线标志)

**问题**: Agent 缺乏对整体地图布局的感知，每次都像"瞎子"一样探索

**解决方案**:
- 在对局开始时，将地图布局编码为 8x8=64 维网格
- 每个格子：1.0=有墙，0.0=空地
- 额外添加1维"直线视线"标志：当前是否有到敌人的直线视线

**状态维度变化**:
```
旧: 9 (基础) + 48 (射线) = 57维
新: 9 (基础) + 64 (地图网格) + 1 (视线标志) + 48 (射线) = 122维
```

**代码位置**: `rl/TankEnv.cc` - `getCurrentState()`

```cpp
// 全局地图网格 (8x8 = 64 cells)
const int MAP_GRID_SIZE = 8;
std::vector<double> map_grid(MAP_GRID_SIZE * MAP_GRID_SIZE, 0.0);

// 标记所有有墙的格子
for (const auto& kv : *blocks)
{
    // ... 计算block覆盖的格子并标记为1.0 ...
}

state.insert(state.end(), map_grid.begin(), map_grid.end());

// 直线视线标志
state.push_back(hasDirectLineToEnemy(my.pos, en.pos) ? 1.0 : 0.0);
```

**预期效果**:
- Agent 从训练开始就"看到"完整地图
- 可以学习全局路径规划
- 理解"绕过障碍物"的概念
- 在随机地图上泛化能力更强

---

## 📊 新奖励函数完整分解

### 终局奖励（最高优先级）
| 事件 | 奖励值 | 说明 |
|------|--------|------|
| 胜利 | +100 | 击败敌人 |
| 失败 | -100 | 被击败 |
| 平局 | 0 | 同时被消灭 |

### 射击相关奖励
| 行为 | 奖励值 | 触发条件 |
|------|--------|----------|
| **直线射击** | **+10** | 精准对准+无障碍物+10s冷却完成 |
| 精确射击（冷却中） | +2.0 | 18度内+有/无障碍物 |
| 一般射击 | +0.5 | 18-54度内 |
| 浪费弹药 | -0.2 | >54度 |
| **频繁射击** | **-1.0** | 3秒内>3次射击 |

### 生存相关奖励
| 行为 | 奖励值 | 说明 |
|------|--------|------|
| 停滞/撞墙 | -0.5 | 移动<0.5像素 |
| 远离子弹 | +0.4 | 子弹在150像素内时远离 |
| 靠近子弹 | -0.8 | 子弹在150像素内时靠近 |

### 战术引导奖励（微调）
| 行为 | 奖励值 | 说明 |
|------|--------|------|
| 靠近敌人 | +0.1 × Δdist | 权重降低，避免自杀冲锋 |
| 对准敌人 | +0.2 × Δangle | 略微提高，鼓励瞄准 |
| 原地旋转 | -0.05 | 移动<1.0且转角>5度 |
| 步骤成本 | -0.01 | 每步固定成本 |

---

## 🎮 使用指南

### 立即开始训练

```bash
cd /home/rachel/CLionProjects/DRL_for_TankTrouble/build
./TankTrouble
# 点击 "Agent训练" 按钮
```

### 观察改进效果

**训练初期（0-100 episodes）**:
- 看到更多"等待-对准-射击"的序列
- 减少无脑冲锋行为
- 偶尔触发 +10 直线射击奖励（终端会打印）

**训练中期（100-500 episodes）**:
- Agent 开始绕过障碍物寻找射击位置
- 射击频率下降，但命中率提升
- 更好的走位和躲避

**训练后期（500+ episodes）**:
- 战术性positioning
- 充分利用地图布局
- 高效的射击时机选择

### 监控关键指标

在终端日志中关注：

```
[AGENT] step=100 action=5 (python) r=10.2 dp=0.1 ap=0.05 sp=0 sc=-0.01
```

- **r > 5**: 可能触发了直线射击奖励！
- **r < -0.5**: 可能触发了频繁射击惩罚或靠近子弹惩罚
- **dp, ap 稳定为正**: Agent 在有效接近和瞄准

---

## 🔧 调优建议

### 如果 Agent 仍然冲锋自杀

```cpp
// 在 rl/TankEnv.cc 中进一步降低"靠近敌人"奖励
r += 0.05 * (prev_dist_norm_ - dist_norm);  // 从0.1降到0.05
```

### 如果 Agent 不敢射击

```cpp
// 降低频繁射击惩罚或增加射击奖励
if (recent_shot_times_.size() > 4)  // 允许3秒内4次射击
    r -= 0.5;  // 降低惩罚力度
```

### 如果直线射击奖励触发太少

```cpp
// 放宽角度要求或缩短冷却时间
if (align_norm_fire < 0.15)  // 从0.1放宽到0.15（27度）
{
    if (current_time - last_direct_shot_time_ >= 5.0)  // 冷却从10s降到5s
```

---

## 📈 预期训练曲线

### 胜率提升
```
Episode 0-100:    20-30% (随机探索，频繁触发惩罚)
Episode 100-300:  30-45% (开始理解直线射击，减少自杀)
Episode 300-500:  45-60% (战术性positioning，合理射击)
Episode 500-1000: 60-75% (利用地图优势，高效对抗)
Episode 1000+:    70-85% (接近最优策略)
```

### 平均奖励
```
初期: -50 到 -20 （频繁失败）
中期: -10 到 +10  （有来有回）
后期: +10 到 +50  （常胜将军）
```

### 直线射击触发率
```
初期: <1% （不会找位置）
中期: 5-10% （偶尔抓住机会）
后期: 20-30% （主动创造机会）
```

---

## 🧪 测试脚本

创建测试脚本验证改进：

```python
# test_new_rewards.py
import tank_trouble_env

env = tank_trouble_env.TankEnv()
state = env.reset()

print(f"新状态维度: {len(state)} (应该是122)")
print(f"地图网格: state[9:73] = {state[9:73][:10]}...")  # 前10个格子
print(f"视线标志: state[73] = {state[73]}")
print(f"射线: state[74:122] = {state[74:84]}...")  # 前10个射线值

# 测试射击奖励
for _ in range(100):
    action = 5  # 一直射击
    next_state, reward, done = env.step(action)
    if reward > 5:
        print(f"🎯 触发直线射击奖励！reward={reward:.2f}")
    elif reward < -0.5:
        print(f"🚫 触发惩罚！reward={reward:.2f}")
    if done:
        break
```

---

## 📚 相关文件

| 文件 | 修改内容 |
|------|----------|
| `rl/TankEnv.h` | 添加时间追踪、射击历史、辅助方法 |
| `rl/TankEnv.cc` | 实现3大改进：直线射击、频繁射击惩罚、地图信息 |
| `controller/RLController.cc` | 更新状态维度到122，添加地图网格构建 |
| `Window.cc` | 更新初始化参数：57→122 |
| `python/train_with_gui.py` | 更新默认状态维度：57→122 |

---

## ✅ 改进总结

### 关键创新

1. **战术射击奖励**: 不仅奖励"射击"，更奖励"在正确位置射击"
2. **节奏控制**: 防止无脑spam，鼓励有策略的火力控制
3. **全局视野**: Agent "天生"就知道地图布局，无需盲目探索

### 解决的核心问题

| 旧问题 | 新方案 | 效果 |
|--------|--------|------|
| 犹豫不决 | 步骤成本+停滞惩罚 | 鼓励果断移动 |
| 自杀冲锋 | 降低接近奖励+子弹躲避 | 保持安全距离 |
| 乱射 | 直线射击奖励+频繁射击惩罚 | 战术性射击 |
| 地图盲目 | 8x8全局网格 | 理解布局，规划路线 |

### 预期改进幅度

- **胜率**: 预计从 40% 提升到 70%+
- **效率**: 平均对局步数减少 30%
- **战术性**: 从"随机游走"到"有策略对抗"

---

## 🎯 下一步

1. **立即训练**: 运行 500+ episodes 观察效果
2. **监控日志**: 关注直线射击触发率和惩罚频率
3. **微调参数**: 根据实际表现调整奖励权重
4. **长期训练**: 1000+ episodes 达到稳定高水平

开始你的训练之旅，见证 Agent 从"菜鸟"蜕变为"战术大师"！🚀

