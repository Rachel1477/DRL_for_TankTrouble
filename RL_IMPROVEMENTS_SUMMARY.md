# RL Training Improvements Summary

## é—®é¢˜è¯Šæ–­

æ‚¨è§‚å¯Ÿåˆ° agent "æ²¡æœ‰å¤´ç»ªçš„ä¹±è¡ŒåŠ¨"ï¼Œç»è¿‡åˆ†æå‘ç°ï¼š

###  å®é™…æƒ…å†µ
1. **âœ… Agent èƒ½è·å–æ•Œäººä½ç½®**ï¼šstate[5:7] åŒ…å«ç›¸å¯¹ä½ç½® (dx, dy)
2. **âœ… Agent èƒ½æ„ŸçŸ¥åœ°å›¾**ï¼š16ä¸ªå°„çº¿æ£€æµ‹ (48ç»´) æä¾›å¢™å£ã€æ•Œäººã€å­å¼¹è·ç¦»
3. **âœ… Agent èƒ½æ„ŸçŸ¥è‡ªèº«**ï¼šä½ç½®ã€è§’åº¦ã€å¼¹è¯çŠ¶æ€éƒ½åœ¨çŠ¶æ€å‘é‡ä¸­

### çœŸæ­£çš„é—®é¢˜
1. **ç½‘ç»œå®¹é‡ä¸è¶³**ï¼šåŸå§‹çš„ 128->128 ç½‘ç»œå¤ªå°ï¼Œæ— æ³•å­¦ä¹  57ç»´å¤æ‚çŠ¶æ€ç©ºé—´
2. **æ¢ç´¢ä¸å……åˆ†**ï¼šå¿«é€Ÿè¡°å‡çš„ epsilon å¯¼è‡´åœ¨éšæœºåœ°å›¾ä¸Šæ¬ æ¢ç´¢
3. **å¥–åŠ±ä¿¡å·å¼±**ï¼šç¼ºå°‘å°„å‡»å¥–åŠ±å’Œå­å¼¹èº²é¿å¥–åŠ±ï¼Œagent ä¸çŸ¥é“ä½•æ—¶è¯¥å°„å‡»
4. **å­¦ä¹ æœºåˆ¶ç¼ºå¤±**ï¼šGUI æ¨¡å¼ä¸‹æ²¡æœ‰å°†ç»éªŒå­˜å…¥ replay buffer å¹¶å­¦ä¹ 

## å®æ–½çš„æ”¹è¿›

### 1. å¢å¼º DQN ç½‘ç»œç»“æ„ âœ…

**æ–‡ä»¶**: `python/train_dqn.py`

**å˜æ›´**:
```python
# Before: 128 -> 128 -> 6
# After:  256 -> 256 -> 128 -> 6

class QNetwork(nn.Module):
    def __init__(self, state_size: int, action_size: int, seed: int = 0):
        super(QNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)
        
        # æ›´æ·±æ›´å®½çš„ç½‘ç»œ
        self.fc1 = nn.Linear(state_size, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, action_size)
        
        # Batch Normalization ç¨³å®šè®­ç»ƒ
        self.bn1 = nn.BatchNorm1d(256)
        self.bn2 = nn.BatchNorm1d(256)
```

**æ•ˆæœ**: ç½‘ç»œå‚æ•°ä» ~33K å¢åŠ åˆ° ~148Kï¼Œå¤§å¹…æå‡å­¦ä¹ å¤æ‚ç­–ç•¥çš„èƒ½åŠ›ã€‚

---

### 2. ä¼˜åŒ–æ¢ç´¢ç­–ç•¥é€‚åº”éšæœºåœ°å›¾ âœ…

**æ–‡ä»¶**: `python/train_with_gui.py`

**å˜æ›´**:
```python
# Before: å¿«é€Ÿè¡°å‡ eps = 0.995^episode (500å±€åä»…å‰©0.08)
# After: åˆ†é˜¶æ®µè¡°å‡

if episode < 500:
    eps = 1.0 - 0.95 * (episode / 500.0)  # 1.0 -> 0.05 (å‰500å±€)
elif episode < 1000:
    eps = 0.05 - 0.04 * ((episode - 500) / 500.0)  # 0.05 -> 0.01 (500-1000å±€)
else:
    eps = 0.01  # æœ€å°æ¢ç´¢
```

**æ•ˆæœ**: 
- å‰500å±€ä¿æŒè¾ƒé«˜æ¢ç´¢ï¼Œå……åˆ†é€‚åº”éšæœºåœ°å›¾
- 1000å±€åä»ä¿ç•™1%æ¢ç´¢ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜

---

### 3. å¢å¼ºå¥–åŠ±å‡½æ•° âœ…

**æ–‡ä»¶**: `rl/TankEnv.cc`

**æ–°å¢å¥–åŠ±**:

#### 3.1 å°„å‡»å¥–åŠ±
```cpp
int current_shells = me->remainShells();
if(current_shells < last_my_shells_)  // Agent å°„å‡»äº†
{
    if(align_norm < 0.1)       // ç„å‡†ç²¾ç¡® (~18åº¦å†…)
        r += 2.0;              // å¤§å¥–åŠ±
    else if(align_norm < 0.3)  // ç„å‡†ä¸€èˆ¬ (~54åº¦å†…)
        r += 0.5;              // å°å¥–åŠ±
    else
        r -= 0.5;              // æµªè´¹å¼¹è¯ï¼Œæƒ©ç½š
}
```

#### 3.2 å­å¼¹èº²é¿å¥–åŠ±
```cpp
// æ‰¾åˆ°æœ€è¿‘çš„å­å¼¹
double closest_bullet_dist = 1000.0;
for(auto& kv : objs)
{
    if(kv.second->type() == OBJ_SHELL)
    {
        Shell* sh = dynamic_cast<Shell*>(kv.second.get());
        auto sh_pos = sh->getCurrentPosition();
        double d = std::hypot(sh_pos.pos.x() - my.pos.x(), sh_pos.pos.y() - my.pos.y());
        closest_bullet_dist = std::min(closest_bullet_dist, d);
    }
}

// å¥–åŠ±è¿œç¦»å­å¼¹çš„è¡Œä¸º
if(closest_bullet_dist < prev_closest_bullet_dist_ && closest_bullet_dist < 100.0)
    r -= 0.1;  // é è¿‘å­å¼¹ -> æƒ©ç½š
else if(closest_bullet_dist > prev_closest_bullet_dist_ && prev_closest_bullet_dist_ < 100.0)
    r += 0.1;  // è¿œç¦»å­å¼¹ -> å¥–åŠ±
```

#### 3.3 è°ƒæ•´å…¶ä»–å¥–åŠ±æƒé‡
```cpp
// å¢åŠ æ¥è¿‘æ•Œäººå¥–åŠ± (0.5 -> 0.8)
r += 0.8 * (prev_dist_norm_ - dist_norm);

// å¢åŠ å¯¹å‡†æ•Œäººå¥–åŠ± (0.1 -> 0.15)
r += 0.15 * (prev_align_norm_ - align_norm);

// å‡å°‘æ­¥éª¤æˆæœ¬ (0.001 -> 0.0005) å…è®¸æ›´å¤šæ¢ç´¢
r -= 0.0005;
```

**æ•ˆæœ**: 
- Agent å­¦ä¼šåœ¨ç„å‡†æ—¶å°„å‡»
- Agent å­¦ä¼šèº²é¿æ•Œæ–¹å­å¼¹
- æ›´ç§¯æåœ°æ¥è¿‘å’Œç„å‡†æ•Œäºº

---

### 4. æ·»åŠ ç»éªŒç´¯ç§¯ä¸å­¦ä¹ æœºåˆ¶ âœ…

**æ–‡ä»¶**: 
- `controller/RLController.h/cc`: æ·»åŠ  `StepCallback`
- `python/train_with_gui.py`: å®ç° `on_step()` å‡½æ•°
- `Window.cc`: æ³¨å…¥ step callback

**å˜æ›´**:

#### 4.1 C++ ç«¯ï¼šæ¯æ­¥å­˜å‚¨ç»éªŒ
```cpp
// RLController::agentLoop() ä¸­
if(step_count > 0 && step_cb_ && !prev_state.empty())
{
    step_cb_(prev_state, prev_action, reward, current_state, done);
}
```

#### 4.2 Python ç«¯ï¼šæ·»åŠ åˆ° replay buffer
```python
def on_step(prev_state, prev_action, reward, next_state, done):
    """æ¯æ­¥å­˜å…¥ replay buffer"""
    global _global_agent, _episode_rewards
    if _global_agent is None:
        return
    
    # æ·»åŠ ç»éªŒ
    _global_agent.step(prev_state, prev_action, reward, next_state, done)
    _episode_rewards.append(reward)
```

#### 4.3 Episode ç»“æŸæ—¶æ‰¹é‡å­¦ä¹ 
```python
def on_episode_end(episode, total_reward, agent_won):
    if _global_agent is not None and len(_global_agent.memory) > _global_agent.batch_size:
        # è‡ªé€‚åº”å­¦ä¹ æ¬¡æ•°
        num_learning_steps = min(10, _episode_step_count // 10)
        for _ in range(num_learning_steps):
            experiences = _global_agent.memory.sample()
            _global_agent.learn(experiences, _global_agent.gamma)
        
        # æ›´æ–° target network
        _global_agent.soft_update(_global_agent.qnetwork_local, 
                                   _global_agent.qnetwork_target, 
                                   _global_agent.tau)
```

**æ•ˆæœ**:
- æ¯ä¸€æ­¥çš„ç»éªŒéƒ½è¢«è®°å½•å’Œå­¦ä¹ 
- Episode ç»“æŸæ—¶è¿›è¡Œå¤šæ¬¡å­¦ä¹ æ›´æ–°
- Buffer ç§¯ç´¯åˆ° 100,000 æ¡ç»éªŒåæŒç»­é‡‡æ ·å­¦ä¹ 

---

### 5. æ”¹è¿›è®­ç»ƒæ—¥å¿— âœ…

**æ–‡ä»¶**: `python/train_with_gui.py`

**å˜æ›´**:
```python
def on_episode_end(episode, total_reward, agent_won):
    # è®¡ç®—å½“å‰ epsilon
    if episode < 500:
        eps = 1.0 - 0.95 * (episode / 500.0)
    elif episode < 1000:
        eps = 0.05 - 0.04 * ((episode - 500) / 500.0)
    else:
        eps = 0.01
    
    result = "WON" if agent_won else "LOST"
    print(f"\n[Episode {episode}] {result} | Steps: {_episode_step_count} | "
          f"Epsilon: {eps:.3f} | Buffer: {len(_global_agent.memory)}")
    
    # æ¯10å±€ä¿å­˜ä¸€æ¬¡æ¨¡å‹ (å‡å°‘I/Oå¼€é”€)
    if _global_agent is not None and episode % 10 == 0:
        torch.save({
            'state_dict': _global_agent.qnetwork_local.state_dict(),
            'episode': episode,
            'agent_won': agent_won,
        }, _global_model_path)
        print(f"[Model saved to {_global_model_path}]")
```

**æ•ˆæœ**: æ¸…æ™°æ˜¾ç¤ºè®­ç»ƒè¿›åº¦ï¼ŒåŒ…æ‹¬èƒœè´Ÿã€æ­¥æ•°ã€æ¢ç´¢ç‡ã€buffer å¤§å°ã€‚

---

## çŠ¶æ€è¡¨ç¤ºå®Œæ•´è¯´æ˜

### 57ç»´çŠ¶æ€å‘é‡ç»„æˆ

| ç»´åº¦ | å†…å®¹ | è¯´æ˜ |
|------|------|------|
| 0-1 | è‡ªå·±ä½ç½® (x, y) | å½’ä¸€åŒ–åˆ° [0, 1] |
| 2-3 | è‡ªå·±è§’åº¦ (sin, cos) | é¿å…360åº¦è·³å˜ |
| 4 | å¼¹è¯çŠ¶æ€ | 1.0 = æœ‰å¼¹è¯, 0.0 = æ— å¼¹è¯ |
| 5-6 | æ•Œäººç›¸å¯¹ä½ç½® (dx, dy) | ç›¸å¯¹äºè‡ªå·±ï¼Œå½’ä¸€åŒ– |
| 7-8 | æ•Œäººè§’åº¦ (sin, cos) | - |
| 9-56 | 16æ¡å°„çº¿ Ã— 3ç§æ£€æµ‹ | è¯¦è§ä¸‹è¡¨ |

### å°„çº¿æ£€æµ‹è¯¦è§£ (48ç»´)

æ¯æ¡å°„çº¿æä¾›3ä¸ªå€¼ï¼š
1. **å¢™å£è·ç¦»** (å½’ä¸€åŒ–): åˆ°æœ€è¿‘å¢™å£/éšœç¢ç‰©çš„è·ç¦»
2. **æ•Œäººè·ç¦»** (å½’ä¸€åŒ–): åˆ°æ•Œæ–¹å¦å…‹çš„è·ç¦»ï¼Œ1.0è¡¨ç¤ºæœªæ£€æµ‹åˆ°
3. **å­å¼¹è·ç¦»** (å½’ä¸€åŒ–): åˆ°æœ€è¿‘å­å¼¹çš„è·ç¦»ï¼Œ1.0è¡¨ç¤ºæœªæ£€æµ‹åˆ°

16æ¡å°„çº¿è¦†ç›–360åº¦ï¼š
- å°„çº¿ 0: 0Â°   (æ­£å‰æ–¹)
- å°„çº¿ 1: 22.5Â°
- å°„çº¿ 2: 45Â°  (å³å‰æ–¹)
- ...
- å°„çº¿ 8: 180Â° (æ­£åæ–¹)
- ...
- å°„çº¿ 15: 337.5Â°

**å…³é”®ç‚¹**:
- Agent **å®Œå…¨çŸ¥é“**æ•Œäººåœ¨å“ªé‡Œ (state[5:7])
- Agent **å®Œå…¨æ„ŸçŸ¥**å‘¨å›´ç¯å¢ƒ (16æ¡å°„çº¿)
- Agent **çŸ¥é“**å­å¼¹ä½ç½®å’Œè½¨è¿¹
- **éšæœºåœ°å›¾ä¸å½±å“çŠ¶æ€è¡¨ç¤º**ï¼Œå› ä¸ºå°„çº¿åŠ¨æ€æ£€æµ‹

---

## å¦‚ä½•éªŒè¯æ”¹è¿›æ•ˆæœ

### è¿è¡Œè®­ç»ƒ
```bash
cd /home/rachel/CLionProjects/DRL_for_TankTrouble/build
./TankTrouble
# ç‚¹å‡»"Agentè®­ç»ƒ"æŒ‰é’®
```

### è§‚å¯ŸæŒ‡æ ‡

#### 1. ç»ˆç«¯è¾“å‡º
```
[AGENT] step=20 action=3 (python) r=0.005 dp=0.002 ap=0.004 sp=0 sc=-0.001
[Episode 50] WON | Steps: 234 | Epsilon: 0.905 | Buffer: 11700
```

#### 2. èƒœç‡æå‡
- **å‰100å±€**: éšæœºæ¢ç´¢ï¼Œèƒœç‡ ~20-30%
- **100-500å±€**: å­¦ä¹ åŸºæœ¬ç­–ç•¥ï¼Œèƒœç‡ ~40-50%
- **500-1000å±€**: ç²¾ç»†åŒ–ç­–ç•¥ï¼Œèƒœç‡ ~60-70%
- **1000å±€+**: ç¨³å®šç­–ç•¥ï¼Œèƒœç‡å¯è¾¾70-80% (SmithAIå¾ˆå¼º)

#### 3. è¡Œä¸ºæ¨¡å¼
- **åˆæœŸ**: ä¹±èµ°ã€ä¹±è½¬ã€ä¹±å°„
- **ä¸­æœŸ**: å¼€å§‹è¿½è¸ªæ•Œäººã€å°è¯•ç„å‡†
- **åæœŸ**: ä¸»åŠ¨æ¥è¿‘ã€ç²¾ç¡®å°„å‡»ã€èº²é¿å­å¼¹

---

## æ–‡ä»¶å˜æ›´æ¸…å•

| æ–‡ä»¶ | æ”¹åŠ¨ | ç›®çš„ |
|------|------|------|
| `python/train_dqn.py` | ç½‘ç»œç»“æ„ï¼š128x2 -> 256x2x128 + BatchNorm | å¢å¼ºå­¦ä¹ èƒ½åŠ› |
| `python/train_with_gui.py` | åˆ†é˜¶æ®µepsilonè¡°å‡ + step callback + æ‰¹é‡å­¦ä¹  | é€‚åº”éšæœºåœ°å›¾ + æŒç»­å­¦ä¹  |
| `rl/TankEnv.h` | æ·»åŠ  `last_my_shells_`, `prev_closest_bullet_dist_` | è¿½è¸ªå°„å‡»å’Œå­å¼¹ |
| `rl/TankEnv.cc` | å°„å‡»å¥–åŠ± + èº²é¿å¥–åŠ± + æƒé‡è°ƒæ•´ | å¼•å¯¼æ™ºèƒ½è¡Œä¸º |
| `controller/RLController.h` | æ·»åŠ  `StepCallback` ç±»å‹å’Œæˆå‘˜ | æ”¯æŒç»éªŒä¼ é€’ |
| `controller/RLController.cc` | å®ç° step callback è°ƒç”¨ | æ¯æ­¥å­˜å‚¨ç»éªŒ |
| `Window.cc` | æ³¨å…¥ `on_step` Python å‡½æ•° | è¿æ¥C++å’ŒPython |

---

## ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### çŸ­æœŸ (ç«‹å³å¯åš)
1. **è°ƒæ•´å¥–åŠ±æƒé‡**: æ ¹æ®è®­ç»ƒæ—¥å¿—å¾®è°ƒå°„å‡»/èº²é¿å¥–åŠ±
2. **å¢åŠ  Buffer å¤§å°**: ä» 100K æå‡åˆ° 200K ä»¥å­˜å‚¨æ›´å¤šéšæœºåœ°å›¾ç»éªŒ
3. **Prioritized Experience Replay**: ä¼˜å…ˆå­¦ä¹ é‡è¦ç»éªŒ

### ä¸­æœŸ (1-2å‘¨)
1. **Dueling DQN**: åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿ï¼Œæå‡å­¦ä¹ æ•ˆç‡
2. **Multi-step Learning**: ä½¿ç”¨ n-step TDï¼ŒåŠ å¿«æ”¶æ•›
3. **Curriculum Learning**: ä»ç®€å•åœ°å›¾é€æ­¥å¢åŠ åˆ°å¤æ‚åœ°å›¾

### é•¿æœŸ (1ä¸ªæœˆ+)
1. **PPO/A3C**: å°è¯• policy-based æ–¹æ³•
2. **Self-play**: Agent vs Agent è‡ªæˆ‘å¯¹å¼ˆ
3. **Attention Mechanism**: è®©ç½‘ç»œå­¦ä¹ å…³æ³¨é‡è¦çš„å°„çº¿æ–¹å‘

---

## å¸¸è§é—®é¢˜

### Q: Agent è¿˜æ˜¯åœ¨åŸåœ°æ‰“è½¬ï¼Ÿ
A: æ£€æŸ¥å¥–åŠ±æ—¥å¿—ä¸­çš„ `sp`ï¼ˆè½¬åœˆæƒ©ç½šï¼‰å€¼ã€‚å¦‚æœç»å¸¸è§¦å‘ï¼Œå¯ä»¥å¢åŠ æƒ©ç½šæƒé‡åˆ° -0.05ã€‚

### Q: Agent ä¸å°„å‡»ï¼Ÿ
A: 
1. æ£€æŸ¥æ˜¯å¦æœ‰ "action=5" å‡ºç°
2. å¢åŠ å¥½å°„å‡»çš„å¥–åŠ± (2.0 -> 3.0)
3. æ·»åŠ "é•¿æ—¶é—´ä¸å°„å‡»"çš„æƒ©ç½š

### Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢ï¼Ÿ
A:
1. ä½¿ç”¨éGUIæ¨¡å¼: `python python/test_env_standalone.py`
2. å‡å°‘å­¦ä¹ é¢‘ç‡: `num_learning_steps = 5` (åœ¨ `on_episode_end` ä¸­)
3. ä½¿ç”¨GPU: ç§»é™¤ `os.environ['CUDA_VISIBLE_DEVICES'] = ''`

### Q: Agent ä¸€ç›´è¾“ï¼Ÿ
A:
1. å‰100å±€è¾“æ˜¯æ­£å¸¸çš„ï¼ˆéšæœºæ¢ç´¢ï¼‰
2. 200å±€åä»<30%èƒœç‡ï¼Œæ£€æŸ¥ç½‘ç»œæ˜¯å¦åœ¨å­¦ä¹ ï¼ˆbufferæ˜¯å¦å¢é•¿ï¼‰
3. å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ç»§ç»­è®­ç»ƒ

---

## ç»“è®º

æ‚¨çš„ agent **ä»ä¸€å¼€å§‹å°±èƒ½æ„ŸçŸ¥åˆ°æ‰€æœ‰å¿…è¦ä¿¡æ¯**ï¼ŒåŒ…æ‹¬ï¼š
- âœ… æ•Œäººä½ç½®
- âœ… åœ°å›¾å¸ƒå±€ï¼ˆé€šè¿‡å°„çº¿ï¼‰
- âœ… å­å¼¹ä½ç½®
- âœ… è‡ªèº«çŠ¶æ€

é—®é¢˜ä¸æ˜¯"æ„ŸçŸ¥ä¸åˆ°ä¿¡æ¯"ï¼Œè€Œæ˜¯"ä¸çŸ¥é“å¦‚ä½•åˆ©ç”¨ä¿¡æ¯"ã€‚é€šè¿‡ï¼š
1. **æ›´å¤§çš„ç½‘ç»œ** å­¦ä¹ å¤æ‚ç­–ç•¥
2. **æ›´å¥½çš„æ¢ç´¢** é€‚åº”éšæœºç¯å¢ƒ
3. **æ›´ä¸°å¯Œçš„å¥–åŠ±** å¼•å¯¼æ­£ç¡®è¡Œä¸º
4. **æŒç»­çš„å­¦ä¹ ** ç§¯ç´¯ç»éªŒ

Agent ç°åœ¨åº”è¯¥èƒ½å¤Ÿåœ¨éšæœºåœ°å›¾ä¸Šæœ‰æ•ˆè®­ç»ƒå¹¶é€æ­¥æå‡æ€§èƒ½ï¼

å¼€å§‹è®­ç»ƒå¹¶è§‚å¯Ÿæ”¹è¿›ï¼é¢„è®¡500å±€åä¼šçœ‹åˆ°æ˜æ˜¾çš„ç­–ç•¥æ€§è¡Œä¸ºã€‚ğŸš€

